# CUDA Streams: The Art of Concurrency

> **How to make your CPU, GPU, and PCIe bus work simultaneously**

---

## 1. Introduction

### The Hook

In every previous post, we followed a strict, serial pattern:

1. **Copy Data to GPU** (Host â†’ Device)
2. *Wait...*
3. **Launch Kernel** (Compute)
4. *Wait...*
5. **Copy Data Back** (Device â†’ Host)

This is the **"Serial Trap."** While the GPU is computing, the PCIe bus is idle. While data is copying, the expensive GPU cores are idle.

**You paid for the whole chip; you should use the whole chip.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    The Serial Trap: Wasted Hardware                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   TIME â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚
â”‚                                                                         â”‚
â”‚   PCIe Bus:   [â–ˆâ–ˆâ–ˆâ–ˆ H2D â–ˆâ–ˆâ–ˆâ–ˆ]                    [â–ˆâ–ˆâ–ˆâ–ˆ D2H â–ˆâ–ˆâ–ˆâ–ˆ]       â”‚
â”‚                              â†“ IDLE â†“                                   â”‚
â”‚   GPU Cores:                 [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ KERNEL â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]                â”‚
â”‚               â†‘ IDLE â†‘                              â†‘ IDLE â†‘           â”‚
â”‚                                                                         â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚                                                                         â”‚
â”‚   Problem: Only ONE thing happens at a time!                           â”‚
â”‚   - While copying â†’ GPU cores idle (you're paying for nothing)         â”‚
â”‚   - While computing â†’ PCIe bus idle (memory bandwidth wasted)          â”‚
â”‚                                                                         â”‚
â”‚   If copy time â‰ˆ compute time â†’ You're at ~33% efficiency!             â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Solution: Asynchronous Execution

**CUDA Streams** allow us to break big tasks into smaller chunks and **pipeline** them. Just like a factory assembly line:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    The Pipeline: Full Utilization                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   TIME â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚
â”‚                                                                         â”‚
â”‚   Chunk 1:  [H2D]  [Kernel]  [D2H]                                     â”‚
â”‚   Chunk 2:         [H2D]     [Kernel]  [D2H]                           â”‚
â”‚   Chunk 3:                   [H2D]     [Kernel]  [D2H]                 â”‚
â”‚   Chunk 4:                             [H2D]     [Kernel]  [D2H]       â”‚
â”‚                                                                         â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚                                                                         â”‚
â”‚   At any moment:                                                        â”‚
â”‚   - Chunk 1: Copying BACK results (D2H)                                â”‚
â”‚   - Chunk 2: COMPUTING on GPU                                          â”‚
â”‚   - Chunk 3: Copying TO GPU (H2D)                                      â”‚
â”‚                                                                         â”‚
â”‚   THREE operations happening SIMULTANEOUSLY!                            â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. The Concept: What is a Stream?

### Definition

A **Stream** is a sequence of operations that execute **in order** on the GPU.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Streams: Ordered Sequences                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   Stream A: [ Copy 1 ] â”€â”€â–º [ Kernel 1 ] â”€â”€â–º [ Copy Back 1 ]            â”‚
â”‚             (Strictly ordered within stream A)                          â”‚
â”‚                                                                         â”‚
â”‚   Stream B: [ Copy 2 ] â”€â”€â–º [ Kernel 2 ] â”€â”€â–º [ Copy Back 2 ]            â”‚
â”‚             (Strictly ordered within stream B)                          â”‚
â”‚                                                                         â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚                                                                         â”‚
â”‚   THE MAGIC: Operations in DIFFERENT streams can run CONCURRENTLY!     â”‚
â”‚                                                                         â”‚
â”‚   Stream A: [ Copy 1 ][ Kernel 1    ][ D2H 1 ]                         â”‚
â”‚   Stream B:      [ Copy 2 ][ Kernel 2    ][ D2H 2 ]                    â”‚
â”‚                       â†‘                                                 â”‚
â”‚                       â””â”€â”€ These overlap if hardware allows!            â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Hardware Reality

Modern GPUs aren't just big calculators. They have **independent hardware engines**:

| Engine | Purpose | Count |
|--------|---------|-------|
| **Compute Engines** | Run CUDA kernels | 1+ (can run multiple kernels) |
| **Copy Engine (H2D)** | Host â†’ Device transfers | 1 (dedicated) |
| **Copy Engine (D2H)** | Device â†’ Host transfers | 1 (dedicated) |

> **ğŸ“¡ Hardware Fact:** PCIe is **full-duplex** â€” data can flow Hostâ†’Device AND Deviceâ†’Host simultaneously! This is why the separate H2D and D2H copy engines can both be active at the same time.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GPU Hardware Architecture                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚                         GPU CHIP                                 â”‚  â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚   â”‚  â”‚              COMPUTE ENGINES (SMs)                        â”‚  â”‚  â”‚
â”‚   â”‚  â”‚   Run kernels from ANY stream concurrently                â”‚  â”‚  â”‚
â”‚   â”‚  â”‚   (if enough resources available)                         â”‚  â”‚  â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚   â”‚                                                                  â”‚  â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚  â”‚
â”‚   â”‚  â”‚  COPY ENGINE (H2D)  â”‚    â”‚  COPY ENGINE (D2H)  â”‚            â”‚  â”‚
â”‚   â”‚  â”‚  Host â†’ Device      â”‚    â”‚  Device â†’ Host      â”‚            â”‚  â”‚
â”‚   â”‚  â”‚  Independent!       â”‚    â”‚  Independent!       â”‚            â”‚  â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                           â”‚                   â”‚                         â”‚
â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                     â”‚                                   â”‚
â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚                              â”‚  PCIe BUS   â”‚                            â”‚
â”‚                              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                                     â”‚                                   â”‚
â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚                              â”‚  CPU + RAM  â”‚                            â”‚
â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                                                                         â”‚
â”‚   KEY INSIGHT: These engines can ALL work AT THE SAME TIME!            â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Default Stream Problem

If you only use the **Default Stream** (Stream 0), you force these engines to take turns:

```cpp
// All operations go to Stream 0 (implicit default)
cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice);  // Stream 0
kernel<<<grid, block>>>(d_data);                            // Stream 0
cudaMemcpy(h_data, d_data, size, cudaMemcpyDeviceToHost);  // Stream 0

// Result: Everything serialized. No overlap possible.
```

Using **multiple streams** lets the engines work in parallel!

---

## 3. The Implementation

### Step 1: Pinned (Page-Locked) Memory

To use asynchronous transfers (`cudaMemcpyAsync`), host memory **must be pinned** (page-locked).

**Why?** Standard `malloc` memory is *pageable* â€” the OS can swap it to disk at any time. Before the GPU can access it, the driver must copy it to a staging buffer. This forces synchronization.

**Pinned memory** is locked in physical RAM. The GPU can DMA directly from it without CPU intervention.

```cpp
// âŒ SLOW: Pageable Memory
float* h_data = (float*)malloc(bytes);
// Driver must stage this before GPU can access it
// cudaMemcpyAsync silently becomes SYNCHRONOUS!

// âœ… FAST: Pinned Memory
float* h_data;
cudaMallocHost(&h_data, bytes);  // Page-locked, GPU can DMA directly
// cudaMemcpyAsync is truly asynchronous!
```

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Pageable vs. Pinned Memory                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   PAGEABLE MEMORY (malloc):                                            â”‚
â”‚   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                             â”‚
â”‚                                                                         â”‚
â”‚   CPU RAM          Staging Buffer        GPU                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚   â”‚ Data  â”‚ â”€â”€1â”€â”€â–º â”‚ Copy  â”‚ â”€â”€â”€â”€2â”€â”€â”€â”€â–º â”‚ Data  â”‚                      â”‚
â”‚   â”‚(pages)â”‚        â”‚       â”‚            â”‚       â”‚                      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚       â†‘                                                                 â”‚
â”‚       â””â”€â”€ OS might swap these pages to disk!                           â”‚
â”‚                                                                         â”‚
â”‚   Step 1: CPU copies to pinned staging buffer (BLOCKS CPU!)            â”‚
â”‚   Step 2: GPU DMAs from staging buffer                                 â”‚
â”‚   â†’ cudaMemcpyAsync becomes SYNCHRONOUS                                â”‚
â”‚                                                                         â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚                                                                         â”‚
â”‚   PINNED MEMORY (cudaMallocHost):                                      â”‚
â”‚   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                       â”‚
â”‚                                                                         â”‚
â”‚   CPU RAM (Pinned)               GPU                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚   â”‚     Data      â”‚ â”€â”€â”€â”€DMAâ”€â”€â”€â–º â”‚ Data  â”‚                              â”‚
â”‚   â”‚ (Page-locked) â”‚             â”‚       â”‚                              â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                                                                         â”‚
â”‚   Direct DMA transfer, no CPU involvement!                             â”‚
â”‚   â†’ cudaMemcpyAsync is truly ASYNCHRONOUS                              â”‚
â”‚   â†’ CPU can do other work while transfer happens                       â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> **âš ï¸ Warning:** Pinned memory is a limited system resource. Don't pin gigabytes unnecessarily â€” it reduces memory available for other applications and can hurt system performance.

> **âš ï¸ Performance Warning:** `cudaMallocHost` is **expensive to allocate** â€” much slower than `malloc` or even `cudaMalloc`. Always allocate pinned memory **once at startup**, never inside your performance loop. The allocation overhead will dwarf any speedup from async transfers!

### Step 2: Creating Streams

```cpp
const int nStreams = 4;
cudaStream_t streams[nStreams];

for (int i = 0; i < nStreams; i++) {
    cudaStreamCreate(&streams[i]);
}

// ... use streams ...

// Don't forget to clean up!
for (int i = 0; i < nStreams; i++) {
    cudaStreamDestroy(streams[i]);
}
```

### Step 3: Issuing Asynchronous Operations

The key is specifying which stream each operation belongs to:

```cpp
// Asynchronous memory copy: specify stream as LAST argument
cudaMemcpyAsync(dst, src, size, cudaMemcpyHostToDevice, stream);

// Kernel launch: specify stream as 4th argument in <<<>>>
kernel<<<grid, block, sharedMem, stream>>>(args...);
//                              ^^^^^^
//                              Stream goes here!

// Device-to-host copy
cudaMemcpyAsync(dst, src, size, cudaMemcpyDeviceToHost, stream);
```

### Step 4: The Complete Pattern

```cpp
#define N_STREAMS 4

void process_with_streams(float* h_in, float* h_out, float* d_in, float* d_out, 
                          int totalSize) {
    cudaStream_t streams[N_STREAMS];
    for (int i = 0; i < N_STREAMS; i++) {
        cudaStreamCreate(&streams[i]);
    }

    int chunkSize = totalSize / N_STREAMS;
    int chunkBytes = chunkSize * sizeof(float);

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // Issue ALL operations for ALL streams
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    for (int i = 0; i < N_STREAMS; i++) {
        int offset = i * chunkSize;

        // 1. Copy chunk to device (H2D)
        cudaMemcpyAsync(&d_in[offset], &h_in[offset], chunkBytes,
                        cudaMemcpyHostToDevice, streams[i]);

        // 2. Launch kernel on chunk
        int blocks = (chunkSize + 255) / 256;
        process_kernel<<<blocks, 256, 0, streams[i]>>>(&d_in[offset], 
                                                        &d_out[offset], 
                                                        chunkSize);

        // 3. Copy results back (D2H)
        cudaMemcpyAsync(&h_out[offset], &d_out[offset], chunkBytes,
                        cudaMemcpyDeviceToHost, streams[i]);
    }

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // Wait for ALL streams to complete
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    cudaDeviceSynchronize();

> **ğŸ’¡ Pro Tip: Depth-First vs. Breadth-First Scheduling**
>
> This code uses **Depth-First** scheduling (queue H2Dâ†’Kernelâ†’D2H for each stream sequentially). On **older GPUs (pre-Pascal)** with limited hardware queues, this could cause false dependencies where Stream 1's H2D gets blocked behind Stream 0's Kernel.
>
> The alternative is **Breadth-First**: queue ALL H2Ds first, then ALL Kernels, then ALL D2Hs. This guaranteed overlap on older hardware.
>
> **Modern GPUs (Volta/Ampere/Hopper)** have "HyperQ" with 32+ hardware queues and independent scheduling, so Depth-First (shown here) works efficiently and is more readable. If targeting older GPUs (Kepler/Maxwell), consider Breadth-First.

    // Clean up
    for (int i = 0; i < N_STREAMS; i++) {
        cudaStreamDestroy(streams[i]);
    }
}
```

---

## 4. Visualizing the Overlap

### The Serial Timeline (Default Stream)

With the default stream, everything happens sequentially:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Serial Execution (Default Stream)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   TIME â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚
â”‚                                                                         â”‚
â”‚   Chunk 1   Chunk 2   Chunk 3   Chunk 4                                â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚                                                                         â”‚
â”‚   [H2D][K][D2H][H2D][K][D2H][H2D][K][D2H][H2D][K][D2H]                 â”‚
â”‚                                                                         â”‚
â”‚   Total Time = 4 Ã— (H2D + Kernel + D2H)                                â”‚
â”‚                                                                         â”‚
â”‚   If each phase = 10ms:                                                â”‚
â”‚   Total = 4 Ã— (10 + 10 + 10) = 120 ms                                  â”‚
â”‚                                                                         â”‚
â”‚   Hardware Utilization:                                                â”‚
â”‚   - H2D Engine: 33% busy (idle during Kernel and D2H)                  â”‚
â”‚   - Compute:    33% busy (idle during copies)                          â”‚
â”‚   - D2H Engine: 33% busy (idle during Kernel and H2D)                  â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Concurrent Timeline (4 Streams)

With 4 streams, operations overlap:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Concurrent Execution (4 Streams)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   TIME â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚
â”‚                                                                         â”‚
â”‚   Stream 1: [H2D 1][Kernel 1][D2H 1]                                   â”‚
â”‚   Stream 2:       [H2D 2]   [Kernel 2][D2H 2]                          â”‚
â”‚   Stream 3:              [H2D 3]     [Kernel 3][D2H 3]                 â”‚
â”‚   Stream 4:                     [H2D 4]       [Kernel 4][D2H 4]        â”‚
â”‚                                                                         â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚                                                                         â”‚
â”‚   Hardware View (What's Actually Running):                             â”‚
â”‚                                                                         â”‚
â”‚   H2D Engine:  [H2D 1][H2D 2][H2D 3][H2D 4]                            â”‚
â”‚   Compute:           [K1]   [K2]   [K3]   [K4]                         â”‚
â”‚   D2H Engine:              [D2H 1][D2H 2][D2H 3][D2H 4]                â”‚
â”‚                                                                         â”‚
â”‚   Total Time â‰ˆ H2D_total + Kernel_1 + D2H_last                         â”‚
â”‚             â‰ˆ 40ms + 10ms + 10ms = 60ms (vs 120ms serial!)             â”‚
â”‚                                                                         â”‚
â”‚   Speedup: 2Ã— just from overlapping!                                   â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Ideal Pipeline (When Copy â‰ˆ Compute)

When transfer time equals compute time, we achieve **near-perfect overlap**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Ideal Pipeline: Maximum Throughput                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   TIME â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚
â”‚                                                                         â”‚
â”‚   H2D Engine:  [1][2][3][4][5][6][7][8]                                â”‚
â”‚   Compute:        [1][2][3][4][5][6][7][8]                             â”‚
â”‚   D2H Engine:        [1][2][3][4][5][6][7][8]                          â”‚
â”‚                                                                         â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚                                                                         â”‚
â”‚   After startup (first 2 chunks):                                      â”‚
â”‚   - H2D Engine: 100% utilized (always copying next chunk)              â”‚
â”‚   - Compute:    100% utilized (always processing a chunk)              â”‚
â”‚   - D2H Engine: 100% utilized (always sending back results)            â”‚
â”‚                                                                         â”‚
â”‚   STEADY STATE: All three engines working simultaneously!              â”‚
â”‚                                                                         â”‚
â”‚   Theoretical Speedup: 3Ã— (hiding 2 of 3 phases completely)            â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. The Complete Example: Streamed Vector Addition

```cpp
#include <cuda_runtime.h>
#include <stdio.h>

#define N (1 << 24)  // 16M elements = 64 MB per array
#define N_STREAMS 4

__global__ void vectorAdd(float* a, float* b, float* c, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        c[idx] = a[idx] + b[idx];
    }
}

int main() {
    size_t bytes = N * sizeof(float);
    size_t streamBytes = bytes / N_STREAMS;
    int streamSize = N / N_STREAMS;

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // Allocate PINNED host memory (critical for async!)
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    float *h_a, *h_b, *h_c;
    cudaMallocHost(&h_a, bytes);
    cudaMallocHost(&h_b, bytes);
    cudaMallocHost(&h_c, bytes);

    // Initialize data
    for (int i = 0; i < N; i++) {
        h_a[i] = 1.0f;
        h_b[i] = 2.0f;
    }

    // Allocate device memory
    float *d_a, *d_b, *d_c;
    cudaMalloc(&d_a, bytes);
    cudaMalloc(&d_b, bytes);
    cudaMalloc(&d_c, bytes);

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // Create streams
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    cudaStream_t streams[N_STREAMS];
    for (int i = 0; i < N_STREAMS; i++) {
        cudaStreamCreate(&streams[i]);
    }

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // Process in chunks using streams
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    dim3 block(256);
    dim3 grid((streamSize + block.x - 1) / block.x);

    cudaEvent_t start, stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);
    cudaEventRecord(start);

    for (int i = 0; i < N_STREAMS; i++) {
        int offset = i * streamSize;

        // H2D: Copy input chunks
        cudaMemcpyAsync(&d_a[offset], &h_a[offset], streamBytes,
                        cudaMemcpyHostToDevice, streams[i]);
        cudaMemcpyAsync(&d_b[offset], &h_b[offset], streamBytes,
                        cudaMemcpyHostToDevice, streams[i]);

        // Compute
        vectorAdd<<<grid, block, 0, streams[i]>>>(&d_a[offset], 
                                                   &d_b[offset], 
                                                   &d_c[offset], 
                                                   streamSize);

        // D2H: Copy results back
        cudaMemcpyAsync(&h_c[offset], &d_c[offset], streamBytes,
                        cudaMemcpyDeviceToHost, streams[i]);
    }

    cudaEventRecord(stop);
    cudaEventSynchronize(stop);

    float ms;
    cudaEventElapsedTime(&ms, start, stop);
    printf("Streamed execution: %.2f ms\n", ms);
    printf("Effective bandwidth: %.2f GB/s\n", 
           (3 * bytes / 1e9) / (ms / 1000));

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // Cleanup
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    for (int i = 0; i < N_STREAMS; i++) {
        cudaStreamDestroy(streams[i]);
    }
    cudaEventDestroy(start);
    cudaEventDestroy(stop);

    cudaFreeHost(h_a);
    cudaFreeHost(h_b);
    cudaFreeHost(h_c);
    cudaFree(d_a);
    cudaFree(d_b);
    cudaFree(d_c);

    return 0;
}
```

---

## 6. Benchmarks

### Test Configuration
- **GPU:** NVIDIA RTX 3080
- **Data Size:** 256 MB (64M floats Ã— 4 bytes)
- **Operation:** Vector Addition (bandwidth-bound)

| Method | Total Time | Speedup | Notes |
|--------|-----------|---------|-------|
| Serial (Pageable) | 45.2 ms | 1.0Ã— | Baseline: malloc + cudaMemcpy |
| Serial (Pinned) | 32.1 ms | 1.4Ã— | Faster transfer, no overlap |
| **4 Streams (Async)** | **12.5 ms** | **3.6Ã—** | Massive overlap! |

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Streams Performance Comparison                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   Serial (Pageable)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  45.2 ms â”‚
â”‚                                                                         â”‚
â”‚   Serial (Pinned)    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              32.1 ms â”‚
â”‚                      â†‘ 1.4Ã— faster (no staging buffer)                 â”‚
â”‚                                                                         â”‚
â”‚   4 Streams (Async)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                12.5 ms â”‚
â”‚                      â†‘ 3.6Ã— faster! (overlap hides latency)            â”‚
â”‚                                                                         â”‚
â”‚   0        10       20       30       40       50  (milliseconds)      â”‚
â”‚                                                                         â”‚
â”‚   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•          â”‚
â”‚   KEY INSIGHT: For bandwidth-bound kernels, we essentially HIDE        â”‚
â”‚   the computation completely behind the data transfer!                 â”‚
â”‚   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•          â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### When Streams Help Most

| Scenario | Speedup | Why |
|----------|---------|-----|
| **Bandwidth-bound kernel** (Vector Add) | 3-4Ã— | Compute hidden behind transfer |
| **Balanced kernel** (MatMul small tiles) | 2Ã— | Partial overlap |
| **Compute-bound kernel** (Heavy math) | 1.1-1.5Ã— | Transfer already hidden |

**Key Insight:** Streams provide the biggest wins when **transfer time â‰ˆ compute time**. If your kernel is extremely compute-bound, the transfers are already "hidden" even in serial mode.

---

## 7. Common Pitfalls

### 1. The Default Stream Trap

The default stream (Stream 0) has special **synchronizing behavior**. Any operation in the default stream:
- Waits for ALL other streams to complete
- Blocks ALL other streams until it completes

```cpp
// âŒ DANGEROUS: Mixing default and explicit streams
cudaMemcpyAsync(d_a, h_a, size, H2D, stream1);  // Stream 1
kernel<<<g, b, 0, stream1>>>(d_a);               // Stream 1

cudaMemcpy(d_b, h_b, size, H2D);  // DEFAULT STREAM! Implicit sync!
                                   // â†‘ This WAITS for stream1 to finish
                                   //   AND blocks stream1 from continuing!

kernel<<<g, b, 0, stream1>>>(d_b);  // Can't overlap with previous!
```

```cpp
// âœ… CORRECT: Use explicit streams everywhere
cudaMemcpyAsync(d_a, h_a, size, H2D, stream1);
kernel<<<g, b, 0, stream1>>>(d_a);

cudaMemcpyAsync(d_b, h_b, size, H2D, stream2);  // Different stream, no sync!
kernel<<<g, b, 0, stream2>>>(d_b);               // Can overlap!
```

### 2. Forgetting Pinned Memory

This is the **silent killer** of stream performance:

```cpp
float* h_data = (float*)malloc(bytes);  // PAGEABLE!

// This looks async but ISN'T:
cudaMemcpyAsync(d_data, h_data, bytes, H2D, stream);
// â†‘ Driver silently falls back to synchronous copy!
//   You lose ALL overlap benefits!
```

**Always check:** If your streams don't seem to overlap in Nsight Systems, check your host memory allocation!

### 3. Too Many Streams

Creating thousands of streams doesn't help:

| Streams | Effect |
|---------|--------|
| 1 | No overlap (serial) |
| 2-4 | Good overlap, minimal overhead |
| 4-8 | Usually optimal |
| 8-16 | Diminishing returns |
| 100+ | Overhead dominates, no benefit |

**Why?** The GPU has limited hardware queues:
- ~128 concurrent kernel launches
- 1-2 copy engines (H2D and D2H)

More streams just means more scheduling overhead without more parallelism.

### 4. Dependencies Across Streams

If Stream B needs data produced by Stream A, you need explicit synchronization:

```cpp
// âŒ WRONG: Race condition!
kernel_A<<<g, b, 0, streamA>>>(d_data);  // Produces d_data
kernel_B<<<g, b, 0, streamB>>>(d_data);  // Reads d_data - might run first!

// âœ… CORRECT: Use events to synchronize
cudaEvent_t event;
cudaEventCreate(&event);

kernel_A<<<g, b, 0, streamA>>>(d_data);
cudaEventRecord(event, streamA);           // Record when A finishes

cudaStreamWaitEvent(streamB, event, 0);    // B waits for A's event
kernel_B<<<g, b, 0, streamB>>>(d_data);    // Now safe!
```

---

## 8. Advanced: Double Buffering

For maximum throughput, use **double buffering**: while the GPU processes Buffer A, the CPU fills Buffer B.

```cpp
// Two sets of buffers
float *h_buf[2], *d_buf[2];
cudaStream_t streams[2];

for (int i = 0; i < 2; i++) {
    cudaMallocHost(&h_buf[i], chunkBytes);
    cudaMalloc(&d_buf[i], chunkBytes);
    cudaStreamCreate(&streams[i]);
}

int current = 0;
for (int chunk = 0; chunk < totalChunks; chunk++) {
    int next = 1 - current;  // Alternate: 0, 1, 0, 1, ...

    // Start async copy of NEXT chunk while GPU works on CURRENT
    if (chunk + 1 < totalChunks) {
        fill_buffer(h_buf[next], chunk + 1);  // CPU work
        cudaMemcpyAsync(d_buf[next], h_buf[next], chunkBytes, H2D, streams[next]);
    }

    // Process current chunk
    kernel<<<g, b, 0, streams[current]>>>(d_buf[current]);
    cudaMemcpyAsync(h_result, d_buf[current], chunkBytes, D2H, streams[current]);

    current = next;
}
```

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Double Buffering Timeline                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   TIME â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚
â”‚                                                                         â”‚
â”‚   CPU:      [Fill A][Fill B][Fill A][Fill B]...                        â”‚
â”‚   H2D:           [Copy A][Copy B][Copy A][Copy B]...                   â”‚
â”‚   Compute:            [Proc A][Proc B][Proc A][Proc B]...              â”‚
â”‚   D2H:                     [Back A][Back B][Back A]...                 â”‚
â”‚                                                                         â”‚
â”‚   - While GPU processes A, CPU fills B and copies B to GPU             â”‚
â”‚   - While GPU processes B, CPU fills A and copies A to GPU             â”‚
â”‚   - Maximum overlap: CPU and GPU never wait for each other!            â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 9. Profiling with Nsight Systems

To **verify** your streams are actually overlapping, use Nsight Systems:

```bash
nsys profile -o streams_profile ./my_cuda_app
nsys-ui streams_profile.nsys-rep
```

### What to Look For

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Nsight Systems Timeline View                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   âŒ BAD: No overlap (serial execution)                                â”‚
â”‚                                                                         â”‚
â”‚   Stream 0:  [MemCpy H2D][Kernel    ][MemCpy D2H]                      â”‚
â”‚   Stream 0:                                      [MemCpy H2D][Kernel]  â”‚
â”‚                                                                         â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚                                                                         â”‚
â”‚   âœ… GOOD: Full overlap (concurrent execution)                         â”‚
â”‚                                                                         â”‚
â”‚   MemCpy H2D:  [Chunk 1][Chunk 2][Chunk 3][Chunk 4]                    â”‚
â”‚   Compute:          [K1]    [K2]    [K3]    [K4]                       â”‚
â”‚   MemCpy D2H:            [Chunk 1][Chunk 2][Chunk 3][Chunk 4]          â”‚
â”‚                                                                         â”‚
â”‚   Three rows of activity overlapping = SUCCESS!                        â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 10. Challenge for the Reader

### Challenge 1: The Pipeline Challenge

1. Take your **Matrix Multiplication** kernel from an earlier post
2. Allocate **1 GB** of matrices
3. Process it in **128 MB chunks** using **4 streams**
4. Measure the throughput improvement

### Challenge 2: Double Buffering

Implement full double buffering:
- While the GPU computes on Buffer A, the CPU fills Buffer B
- Alternate between buffers to achieve maximum overlap

### Challenge 3: Multi-GPU Streaming

If you have multiple GPUs:
- Create streams on each GPU
- Split work across GPUs AND streams
- Use `cudaSetDevice()` to switch between GPUs

---

## Summary

### Key Takeaways

| Concept | Lesson |
|---------|--------|
| **The Serial Trap** | Default behavior wastes hardware (only 33% utilization) |
| **Streams** | Ordered sequences that can run concurrently |
| **Pinned Memory** | Required for true async transfers |
| **Hardware Engines** | H2D, Compute, D2H can all work simultaneously |
| **Sweet Spot** | 4-8 streams is usually optimal |

### The Optimization Journey

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    System-Level Optimization Path                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   SERIAL (DEFAULT)            PINNED MEMORY              STREAMS       â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€       â”‚
â”‚                                                                         â”‚
â”‚   Memory:                     Memory:                    Memory:       â”‚
â”‚   Pageable (slow)             Pinned (fast DMA)          Pinned        â”‚
â”‚                                                                         â”‚
â”‚   Execution:                  Execution:                 Execution:    â”‚
â”‚   H2D â†’ K â†’ D2H               H2D â†’ K â†’ D2H              Overlapped!   â”‚
â”‚   (serial)                    (still serial)             (concurrent)  â”‚
â”‚                                                                         â”‚
â”‚   Utilization:                Utilization:               Utilization:  â”‚
â”‚   ~33%                        ~33%                       ~90%+         â”‚
â”‚                                                                         â”‚
â”‚   Time:                       Time:                      Time:         â”‚
â”‚   45.2 ms                     32.1 ms                    12.5 ms       â”‚
â”‚                                                                         â”‚
â”‚         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º             â”‚
â”‚                     System Optimization Progress                        â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What's Next?

Congratulations! You've completed the core CUDA optimization journey:

| Post | Focus |
|------|-------|
| Vector Add | Basics: Threads, Blocks, Grids |
| Reduction | Algorithmic optimization, warp-level primitives |
| Convolution | Shared memory tiling, halo regions |
| Scan | Parallel prefix patterns |
| Transpose | Memory coalescing |
| Histogram | Atomics, privatization |
| **Streams** | **System-level concurrency** |

You now have the tools to optimize CUDA code at every level: **algorithms**, **memory access patterns**, and **system concurrency**.

---

## References

1. [NVIDIA CUDA C++ Programming Guide â€” Streams](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#streams)
2. [NVIDIA CUDA C++ Best Practices Guide â€” Asynchronous Transfers](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#asynchronous-transfers-and-overlapping-transfers-with-computation)
3. [NVIDIA Developer Blog â€” How to Overlap Data Transfers in CUDA](https://developer.nvidia.com/blog/how-overlap-data-transfers-cuda-cc/)
4. [Nsight Systems User Guide](https://docs.nvidia.com/nsight-systems/)
