# Convolution in CUDA: How Neural Networks "See"

> **Mastering spatial locality, constant memory, and the "Halo" problem**

---

## 1. Introduction

### The Hook

In our previous posts, we optimized point-wise operations ([Vector Add](../vector%20addition/)) and global aggregations ([Reduction](../Reduction/02_reduction.md)). Now, we tackle the most important operation in Computer Vision: **Convolution**.

If Matrix Multiplication is the *brain* of Deep Learning (Dense layers), Convolution is the *eyes* (CNNs). It allows networks to detect edges, textures, and objects by looking at **local neighborhoods** of pixels.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     The Deep Learning "Senses"                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   ğŸ§  Matrix Multiplication          ğŸ‘ï¸ Convolution                      â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                   â”‚
â”‚   â€¢ Dense/Linear layers            â€¢ CNN layers                         â”‚
â”‚   â€¢ Global connections             â€¢ Local neighborhoods                â”‚
â”‚   â€¢ "Thinking"                     â€¢ "Seeing"                           â”‚
â”‚                                                                         â”‚
â”‚   Input â†’ [Wâ‚Ã—Wâ‚‚Ã—...Ã—Wâ‚™] â†’ Output  Image â†’ [Filter] â†’ Feature Map     â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Problem

Convolution is computationally dense but **memory-heavy**.

For every single pixel in the output, we must read a grid of neighbors (e.g., $3 \times 3$ or $7 \times 7$) from the input.

| Approach | Description | Problem |
|----------|-------------|---------|
| **Naive CPU** | Nested loops heaven (4 loops!) | Slow |
| **Naive GPU** | Massive redundant memory reads | Neighboring threads read almost the same data |

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    The Redundant Read Problem                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   Thread 0 reads:  [A B C]     Thread 1 reads:  [B C D]                â”‚
â”‚                    [E F G]                      [F G H]                â”‚
â”‚                    [I J K]                      [J K L]                â”‚
â”‚                                                                         â”‚
â”‚   Notice: B, C, F, G, J, K are read by BOTH threads!                   â”‚
â”‚   With 1920Ã—1080 pixels, this means BILLIONS of redundant reads        â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Solution

To make this fast, we need to solve **two problems**:

| Resource | Problem | Solution |
|----------|---------|----------|
| **The Weights** | Filter kernel is small, accessed by everyone | **Constant Memory** |
| **The Pixels** | Neighbors share data | **Shared Memory** (handle the "Halo") |

---

## 2. The Algorithm: 2D Stencil

### How It Works

A convolution applies a small **filter** (mask/kernel) to every pixel in an image:

$$\text{Output}[y][x] = \sum_{j=-r}^{r} \sum_{i=-r}^{r} \text{Image}[y+j][x+i] \times \text{Mask}[j+r][i+r]$$

Where $r$ is the **radius** of the filter (for a $3 \times 3$ filter, $r = 1$).

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      2D Convolution Operation                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   Input Image (5Ã—5)              Filter (3Ã—3)          Output (5Ã—5)    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ 1  2  3  4  5   â”‚            â”‚ 1  0 -1 â”‚          â”‚             â”‚  â”‚
â”‚   â”‚ 6  7  8  9  10  â”‚     âŠ›      â”‚ 2  0 -2 â”‚    =     â”‚     ...     â”‚  â”‚
â”‚   â”‚ 11 12 13 14 15  â”‚            â”‚ 1  0 -1 â”‚          â”‚             â”‚  â”‚
â”‚   â”‚ 16 17 18 19 20  â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚      â†“      â”‚  â”‚
â”‚   â”‚ 21 22 23 24 25  â”‚                                 â”‚  Output[2,2]â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                         â”‚
â”‚   For Output[2,2]:                                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                          â”‚
â”‚   â”‚ 7  8  9 â”‚  Ã—  â”‚ 1  0 -1 â”‚  =  7Ã—1 + 8Ã—0 + 9Ã—(-1)                  â”‚
â”‚   â”‚12 13 14 â”‚     â”‚ 2  0 -2 â”‚  + 12Ã—2 + 13Ã—0 + 14Ã—(-2)                â”‚
â”‚   â”‚17 18 19 â”‚     â”‚ 1  0 -1 â”‚  + 17Ã—1 + 18Ã—0 + 19Ã—(-1) = 0            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                          â”‚
â”‚                                                                         â”‚
â”‚   This specific filter is a "Sobel Edge Detector" (horizontal)         â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Common Convolution Filters

| Filter | Size | Purpose | Example Values |
|--------|------|---------|----------------|
| **Box Blur** | 3Ã—3 | Smoothing | All 1/9 |
| **Gaussian Blur** | 3Ã—3, 5Ã—5, 7Ã—7 | Smooth noise | Bell curve weights |
| **Sobel X** | 3Ã—3 | Vertical edges | [[-1,0,1], [-2,0,2], [-1,0,1]] |
| **Sobel Y** | 3Ã—3 | Horizontal edges | [[-1,-2,-1], [0,0,0], [1,2,1]] |
| **Sharpen** | 3Ã—3 | Edge enhancement | [[0,-1,0], [-1,5,-1], [0,-1,0]] |
| **Laplacian** | 3Ã—3 | Edge detection | [[0,1,0], [1,-4,1], [0,1,0]] |

### The "Halo" (Ghost Cells)

This introduces a unique challenge for parallelization. If we divide the image into $16 \times 16$ blocks:

**Problem:** Threads at the edge of the block need pixels from the *next* block to compute their result.

This border region is called the **Halo** or **Apron**.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         The Halo Problem                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚                    What we WANT to compute: 16Ã—16                       â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚                    â”‚                         â”‚                          â”‚
â”‚                    â”‚     OUTPUT TILE         â”‚                          â”‚
â”‚                    â”‚       (16Ã—16)           â”‚                          â”‚
â”‚                    â”‚                         â”‚                          â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                                                                         â”‚
â”‚                    What we NEED to load: 18Ã—18 (for 3Ã—3 filter)        â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚              â”‚ â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚ â† Halo (top)         â”‚
â”‚              â”‚ â–‘â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â–‘â–‘â–‘â–‘ â”‚                       â”‚
â”‚              â”‚ â–‘â”‚                         â”‚â–‘â–‘â–‘â–‘ â”‚                       â”‚
â”‚              â”‚ â–‘â”‚     OUTPUT TILE         â”‚â–‘â–‘â–‘â–‘ â”‚ â† Halo (sides)       â”‚
â”‚              â”‚ â–‘â”‚       (16Ã—16)           â”‚â–‘â–‘â–‘â–‘ â”‚                       â”‚
â”‚              â”‚ â–‘â”‚                         â”‚â–‘â–‘â–‘â–‘ â”‚                       â”‚
â”‚              â”‚ â–‘â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â–‘â–‘â–‘â–‘ â”‚                       â”‚
â”‚              â”‚ â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚ â† Halo (bottom)      â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                           â†‘                                             â”‚
â”‚                      Halo width = filter_radius = 1                     â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**We must load this extra data into Shared Memory effectively.**

---

## 3. Optimization 1: Constant Memory

### The Concept

| Memory Type | Size | Speed | Use Case |
|-------------|------|-------|----------|
| **Global Memory** | Huge (GBs) | Slow (~500 GB/s) | General data |
| **Shared Memory** | Small (48-164 KB/SM) | Fast (~10 TB/s) | Thread cooperation |
| **Constant Memory** | 64 KB | Fast (broadcast) | Read-only, uniform access |

**Constant Memory** (`__constant__`) is a special read-only cache optimized for **broadcasts**:

- **Scenario:** Every single thread reads the same filter weight at the same time
- **Mechanism:** Constant memory broadcasts this single value to all threads in a warp simultaneously
- **Total Space:** 64 KB addressable per kernel (plenty for even large filters)
- **SM Cache:** Each SM has a dedicated 8-10 KB constant cache for ultra-fast access

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Constant Memory Broadcast                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   WITHOUT Constant Memory (Global Memory):                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚   â”‚  Thread 0 â”€â”€â†’ read mask[0] â”€â”€â†’ DRAM     â”‚  32 separate             â”‚
â”‚   â”‚  Thread 1 â”€â”€â†’ read mask[0] â”€â”€â†’ DRAM     â”‚  memory requests!        â”‚
â”‚   â”‚  Thread 2 â”€â”€â†’ read mask[0] â”€â”€â†’ DRAM     â”‚                          â”‚
â”‚   â”‚    ...                                   â”‚                          â”‚
â”‚   â”‚  Thread 31 â”€â†’ read mask[0] â”€â”€â†’ DRAM     â”‚                          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                                                                         â”‚
â”‚   WITH Constant Memory (Broadcast):                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚   â”‚                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚                          â”‚
â”‚   â”‚  Thread 0  â†â”€â”€â”€â”¤          â”‚              â”‚  1 memory request,      â”‚
â”‚   â”‚  Thread 1  â†â”€â”€â”€â”¤ mask[0]  â”‚â†â”€â”€ L1 Cache  â”‚  broadcast to all!      â”‚
â”‚   â”‚  Thread 2  â†â”€â”€â”€â”¤          â”‚              â”‚                          â”‚
â”‚   â”‚    ...     â†â”€â”€â”€â”¤          â”‚              â”‚                          â”‚
â”‚   â”‚  Thread 31 â†â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚                          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                                                                         â”‚
â”‚   Speedup: 32Ã— fewer memory transactions for filter access!            â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Declaration

```cpp
// Stored in special GPU memory (visible to all threads, all blocks)
__constant__ float c_mask[MASK_DIM * MASK_DIM];

// Copy from host to constant memory
cudaMemcpyToSymbol(c_mask, h_mask, MASK_DIM * MASK_DIM * sizeof(float));
```

**Key Points:**
- Declared at **file scope** (outside any function)
- Must know size at **compile time**
- Copied using `cudaMemcpyToSymbol()`, not regular `cudaMemcpy()`

---

## 4. Optimization 2: Tiled Convolution with Halo

### The Strategy

We cannot just load a $16 \times 16$ tile like we did in Matrix Multiplication.

For a $3 \times 3$ filter (radius 1), a $16 \times 16$ output block needs an $18 \times 18$ input block.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Tile Size Calculation                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   Formula: INPUT_TILE = OUTPUT_TILE + 2 Ã— RADIUS                       â”‚
â”‚                                                                         â”‚
â”‚   Example with TILE_SIZE = 16, MASK_DIM = 3 (radius = 1):              â”‚
â”‚                                                                         â”‚
â”‚   Input tile needed: 16 + 2Ã—1 = 18Ã—18 = 324 pixels                     â”‚
â”‚   Output tile produced: 16Ã—16 = 256 pixels                              â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚   â”‚         18 pixels wide                  â”‚                           â”‚
â”‚   â”‚    â”Œâ”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”      â”‚                           â”‚
â”‚   â”‚    â”‚ H â”‚                     â”‚ H â”‚      â”‚                           â”‚
â”‚   â”‚    â”œâ”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”¤      â”‚                           â”‚
â”‚   â”‚ 18 â”‚   â”‚                     â”‚   â”‚      â”‚                           â”‚
â”‚   â”‚ px â”‚   â”‚   16Ã—16 OUTPUT      â”‚   â”‚      â”‚                           â”‚
â”‚   â”‚highâ”‚   â”‚    (computed)       â”‚   â”‚      â”‚                           â”‚
â”‚   â”‚    â”‚   â”‚                     â”‚   â”‚      â”‚                           â”‚
â”‚   â”‚    â”œâ”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”¤      â”‚                           â”‚
â”‚   â”‚    â”‚ H â”‚                     â”‚ H â”‚      â”‚                           â”‚
â”‚   â”‚    â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”˜      â”‚                           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚             H = Halo region (radius = 1)                                â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Loading Dance

There are two main strategies for loading tiles with halos:

#### Strategy 1: Oversize Block (Simple)

Launch more threads than output pixels. The "extra" threads only load data.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Strategy 1: Oversize Block                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   Launch: 18Ã—18 = 324 threads per block                                â”‚
â”‚   Output: 16Ã—16 = 256 pixels                                            â”‚
â”‚                                                                         â”‚
â”‚   Thread Role:                                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚   â”‚ L L L L L L L L L L L L L L L L L L â”‚   â”‚                           â”‚
â”‚   â”‚ L â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” L â”‚   â”‚  L = Load only           â”‚
â”‚   â”‚ L â”‚ C C C C C C C C C C C C C C â”‚ L â”‚   â”‚  C = Compute + Load      â”‚
â”‚   â”‚ L â”‚ C C C C C C C C C C C C C C â”‚ L â”‚   â”‚                           â”‚
â”‚   â”‚   â”‚ ... 16Ã—16 computing threads â”‚   â”‚   â”‚                           â”‚
â”‚   â”‚ L â”‚ C C C C C C C C C C C C C C â”‚ L â”‚   â”‚                           â”‚
â”‚   â”‚ L â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ L â”‚   â”‚                           â”‚
â”‚   â”‚ L L L L L L L L L L L L L L L L L L â”‚   â”‚                           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                                                                         â”‚
â”‚   Pros: Simple logic                                                    â”‚
â”‚   Cons: 68 "wasted" threads (26% overhead)                             â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Strategy 2: Complex Loading (Efficient)

Launch exactly output-sized block. Each thread may load multiple elements.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Strategy 2: Complex Loading                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   Launch: 16Ã—16 = 256 threads per block                                â”‚
â”‚   Load: 18Ã—18 = 324 elements (some threads load 2 elements)            â”‚
â”‚                                                                         â”‚
â”‚   Phase 1: Each thread loads its "main" pixel                          â”‚
â”‚   Phase 2: Border threads load halo pixels                              â”‚
â”‚                                                                         â”‚
â”‚   Example for thread (0,0):                                             â”‚
â”‚   - Loads center pixel at global (bx*16, by*16)                        â”‚
â”‚   - Also loads halo pixel at (bx*16-1, by*16-1)                        â”‚
â”‚                                                                         â”‚
â”‚   Pros: No wasted threads, better occupancy                            â”‚
â”‚   Cons: Complex boundary logic, potential load imbalance               â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**We'll use Strategy 1 (Oversize Block) for clarity in this tutorial.**

---

## 5. The Implementation

### Version 1: Naive (Global Memory Only)

```cpp
__global__ void convolution_naive(float* input, float* output, 
                                   float* mask, int width, int height) {
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    
    if (row < height && col < width) {
        float sum = 0.0f;
        
        // For each filter element
        for (int i = 0; i < MASK_DIM; i++) {
            for (int j = 0; j < MASK_DIM; j++) {
                int img_row = row + i - MASK_RADIUS;
                int img_col = col + j - MASK_RADIUS;
                
                // Boundary check (zero padding)
                if (img_row >= 0 && img_row < height && 
                    img_col >= 0 && img_col < width) {
                    sum += input[img_row * width + img_col] * mask[i * MASK_DIM + j];
                }
            }
        }
        output[row * width + col] = sum;
    }
}
```

**Problems:**
1. Filter (`mask`) read from slow global memory every time
2. Same pixels read multiple times by neighboring threads
3. No data reuse whatsoever

### Version 2: Constant Memory for Filter

```cpp
#define MASK_DIM 3
#define MASK_RADIUS (MASK_DIM / 2)

// Stored in constant memory - accessible by all threads
__constant__ float c_mask[MASK_DIM * MASK_DIM];

__global__ void convolution_const(float* input, float* output, 
                                   int width, int height) {
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    
    if (row < height && col < width) {
        float sum = 0.0f;
        
        for (int i = 0; i < MASK_DIM; i++) {
            for (int j = 0; j < MASK_DIM; j++) {
                int img_row = row + i - MASK_RADIUS;
                int img_col = col + j - MASK_RADIUS;
                
                if (img_row >= 0 && img_row < height && 
                    img_col >= 0 && img_col < width) {
                    // Now using constant memory for mask
                    sum += input[img_row * width + img_col] * c_mask[i * MASK_DIM + j];
                }
            }
        }
        output[row * width + col] = sum;
    }
}
```

**Improvement:** Filter access is now cached and broadcast.

### Version 3: Tiled with Shared Memory (Final)

```cpp
#define MASK_DIM 3
#define MASK_RADIUS (MASK_DIM / 2)
#define TILE_SIZE 16
#define BLOCK_SIZE (TILE_SIZE + MASK_DIM - 1)  // 18 for 3Ã—3 filter

__constant__ float c_mask[MASK_DIM * MASK_DIM];

__global__ void convolution_tiled(float* input, float* output, 
                                   int width, int height) {
    // 1. Shared Memory for the Input Tile (includes Halo)
    __shared__ float s_tile[BLOCK_SIZE][BLOCK_SIZE];

    // 2. Thread indices
    int tx = threadIdx.x;
    int ty = threadIdx.y;
    
    // 3. Global coordinates (offset by radius for halo)
    // We launch 18Ã—18 threads, mapping to an 18Ã—18 region
    int col = blockIdx.x * TILE_SIZE + tx - MASK_RADIUS;
    int row = blockIdx.y * TILE_SIZE + ty - MASK_RADIUS;

    // 4. Load Data (Handle Boundary Checks)
    // All 18Ã—18 threads load one element each
    if (row >= 0 && row < height && col >= 0 && col < width)
        s_tile[ty][tx] = input[row * width + col];
    else
        s_tile[ty][tx] = 0.0f;  // Zero padding for boundaries

    __syncthreads();  // Wait for all threads to finish loading

    // 5. Compute (Only for the inner 16Ã—16 threads)
    // Outer threads (halo loaders) skip this step
    if (tx >= MASK_RADIUS && tx < BLOCK_SIZE - MASK_RADIUS &&
        ty >= MASK_RADIUS && ty < BLOCK_SIZE - MASK_RADIUS) {
        
        float sum = 0.0f;
        
        // Apply the filter using shared memory
        #pragma unroll
        for (int i = 0; i < MASK_DIM; i++) {
            #pragma unroll
            for (int j = 0; j < MASK_DIM; j++) {
                sum += s_tile[ty + i - MASK_RADIUS][tx + j - MASK_RADIUS] 
                     * c_mask[i * MASK_DIM + j];
            }
        }
        
        // Write to global memory
        int out_row = blockIdx.y * TILE_SIZE + (ty - MASK_RADIUS);
        int out_col = blockIdx.x * TILE_SIZE + (tx - MASK_RADIUS);
        
        if (out_row < height && out_col < width)
            output[out_row * width + out_col] = sum;
    }
}
```

### Kernel Launch

```cpp
// Setup
dim3 blockDim(BLOCK_SIZE, BLOCK_SIZE);  // 18Ã—18 threads

// Grid is based on OUTPUT tiles (TILE_SIZE), not input block size!
// Each block of 18Ã—18 threads produces 16Ã—16 output pixels
dim3 gridDim(
    (width + TILE_SIZE - 1) / TILE_SIZE,   // Number of 16-wide output tiles
    (height + TILE_SIZE - 1) / TILE_SIZE   // Number of 16-tall output tiles
);

// Copy filter to constant memory
cudaMemcpyToSymbol(c_mask, h_mask, MASK_DIM * MASK_DIM * sizeof(float));

// Launch
convolution_tiled<<<gridDim, blockDim>>>(d_input, d_output, width, height);
```

---

## 6. Understanding the Memory Access Pattern

### Why Tiling Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Memory Access Comparison                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   NAIVE APPROACH (No Tiling):                                           â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                           â”‚
â”‚   For 16Ã—16 output, each pixel reads 3Ã—3 = 9 neighbors                 â”‚
â”‚   Total reads: 16 Ã— 16 Ã— 9 = 2,304 global memory reads                 â”‚
â”‚   Many are duplicates!                                                  â”‚
â”‚                                                                         â”‚
â”‚   TILED APPROACH:                                                       â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                       â”‚
â”‚   Load 18Ã—18 = 324 elements to shared memory (once)                    â”‚
â”‚   Compute 16Ã—16 outputs using fast shared memory                       â”‚
â”‚   Total global reads: 324                                               â”‚
â”‚                                                                         â”‚
â”‚   Reduction: 2304 â†’ 324 = 7Ã— fewer global memory accesses!             â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚   â”‚  Naive:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 2304     â”‚            â”‚
â”‚   â”‚  Tiled:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  324                                    â”‚            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Reuse Factor

For a $K \times K$ filter and $T \times T$ tile:

$$\text{Reuse Factor} = \frac{T^2 \times K^2}{(T + K - 1)^2}$$

| Tile Size | Filter Size | Naive Reads | Tiled Reads | Reuse Factor |
|-----------|-------------|-------------|-------------|--------------|
| 16Ã—16 | 3Ã—3 | 2,304 | 324 | 7.1Ã— |
| 16Ã—16 | 5Ã—5 | 6,400 | 400 | 16Ã— |
| 16Ã—16 | 7Ã—7 | 12,544 | 484 | 26Ã— |
| 32Ã—32 | 3Ã—3 | 9,216 | 1,156 | 8Ã— |

**Larger filters benefit MORE from tiling!**

---

## 7. Benchmarks

### Test Configuration
- **Image:** 1920Ã—1080 (Full HD), single channel float
- **Filter:** 3Ã—3 Gaussian blur
- **GPU:** NVIDIA RTX 3080
- **Iterations:** 1000 (averaged)

| Implementation | Time (ms) | Speedup | Notes |
|----------------|-----------|---------|-------|
| CPU (OpenCV) | 15.0 | 1Ã— | Optimized C++, single-threaded |
| CPU (OpenCV, 8 threads) | 3.2 | 4.7Ã— | Multi-threaded |
| GPU (Naive Global) | 2.1 | 7Ã— | Bandwidth bound |
| GPU (Constant Memory) | 1.4 | 11Ã— | L2 cache helps pixels |
| GPU (Shared Memory) | 0.9 | 17Ã— | Minimized global reads |
| GPU (cuDNN) | 0.3 | 50Ã— | Highly optimized library |

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Performance Comparison                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   CPU (1 thread)    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  15.0 ms  â”‚
â”‚   CPU (8 threads)   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   3.2 ms  â”‚
â”‚   GPU Naive         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                      2.1 ms  â”‚
â”‚   GPU Constant      â–ˆâ–ˆâ–ˆâ–ˆ                                       1.4 ms  â”‚
â”‚   GPU Shared        â–ˆâ–ˆ                                         0.9 ms  â”‚
â”‚   cuDNN             â–ˆ                                          0.3 ms  â”‚
â”‚                                                                         â”‚
â”‚   0        5        10        15        20  (milliseconds)              â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Scaling with Filter Size

| Filter Size | Naive (ms) | Tiled (ms) | Speedup |
|-------------|------------|------------|---------|
| 3Ã—3 | 2.1 | 0.9 | 2.3Ã— |
| 5Ã—5 | 5.8 | 1.1 | 5.3Ã— |
| 7Ã—7 | 11.2 | 1.4 | 8Ã— |
| 9Ã—9 | 18.5 | 1.8 | 10Ã— |

**Observation:** Tiling benefits increase with filter size due to higher data reuse.

---

## 8. Common Pitfalls

### 1. The "Ghost" Data Bug

```cpp
// âŒ WRONG: blockDim equals tileDim
#define TILE_SIZE 16
dim3 blockDim(TILE_SIZE, TILE_SIZE);  // Only 16Ã—16 threads!

// âœ… CORRECT: blockDim includes halo
#define BLOCK_SIZE (TILE_SIZE + MASK_DIM - 1)  // 18
dim3 blockDim(BLOCK_SIZE, BLOCK_SIZE);  // 18Ã—18 threads
```

**Forgetting that blockDim must be larger than tileDim causes missing halo data.**

### 2. Shared Memory Bank Conflicts

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Bank Conflict Awareness                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   Shared memory has 32 banks (4-byte stride)                           â”‚
â”‚   Bank index = (Address / 4) % 32                                      â”‚
â”‚                                                                         â”‚
â”‚   Our 18Ã—18 tile is actually SAFE:                                     â”‚
â”‚   â€¢ Row 0, Col 0 â†’ Bank 0                                              â”‚
â”‚   â€¢ Row 1, Col 0 â†’ Bank 18                                             â”‚
â”‚   â€¢ Row 2, Col 0 â†’ Bank (36 % 32) = 4                                  â”‚
â”‚   â€¢ Row 3, Col 0 â†’ Bank (54 % 32) = 22                                 â”‚
â”‚   Since 18 is not a multiple of 32, no column-wise bank conflicts!    â”‚
â”‚                                                                         â”‚
â”‚   âš ï¸  However, if your tile width IS a multiple of 32 (e.g., 32Ã—32):  â”‚
â”‚   s_tile[0][0], s_tile[1][0], s_tile[2][0]... all map to BANK 0        â”‚
â”‚   â†’ 32-way bank conflict when threads access same column!              â”‚
â”‚                                                                         â”‚
â”‚   Solution for power-of-2 tiles: Pad the shared memory array           â”‚
â”‚   __shared__ float s_tile[BLOCK_SIZE][BLOCK_SIZE + 1];  // +1 padding  â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. Constant Memory Limits

```cpp
// âŒ WRONG: Large filter in constant memory
#define MASK_DIM 64
__constant__ float c_mask[64 * 64];  // 16 KB - might exceed limit

// âœ… BETTER: Use texture memory or shared memory for very large filters
// Or preload to shared memory at kernel start
```

**Constant memory is limited to 64 KB. For filters larger than ~31Ã—31, use alternatives.**

### 4. Boundary Condition Bugs

```cpp
// âŒ WRONG: Incorrect boundary check
if (row < height && col < width)  // Missing negative check!

// âœ… CORRECT: Full boundary check
if (row >= 0 && row < height && col >= 0 && col < width)
```

---

## 9. Advanced: Separable Convolution

### The Concept

Many common filters are **separable** â€” they can be decomposed into two 1D passes:

$$\text{2D Filter} = \text{Row Filter} \times \text{Column Filter}$$

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Separable Convolution                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   Example: 3Ã—3 Box Blur                                                 â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚
â”‚   â”‚ 1 1 1   â”‚      â”‚ 1 â”‚   â”‚         â”‚                                 â”‚
â”‚   â”‚ 1 1 1   â”‚  =   â”‚ 1 â”‚ Ã— â”‚ 1  1  1 â”‚                                 â”‚
â”‚   â”‚ 1 1 1   â”‚      â”‚ 1 â”‚   â”‚         â”‚                                 â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚
â”‚    2D (KÂ²)       Column(K)   Row(K)                                    â”‚
â”‚                                                                         â”‚
â”‚   Operations:                                                           â”‚
â”‚   â€¢ Non-separable: KÂ² = 9 multiplications per pixel                    â”‚
â”‚   â€¢ Separable: 2K = 6 multiplications per pixel                        â”‚
â”‚                                                                         â”‚
â”‚   For 7Ã—7: Non-separable = 49, Separable = 14  (3.5Ã— fewer ops!)       â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Implementation Strategy

```cpp
// Pass 1: Horizontal (Row) convolution
// Input â†’ Intermediate
convolution_row<<<gridDim, blockDim>>>(d_input, d_temp, width, height);

// Pass 2: Vertical (Column) convolution  
// Intermediate â†’ Output
convolution_col<<<gridDim, blockDim>>>(d_temp, d_output, width, height);
```

### Complexity Comparison

| Filter Size | Non-Separable | Separable | Savings |
|-------------|---------------|-----------|---------|
| 3Ã—3 | 9 ops | 6 ops | 33% |
| 5Ã—5 | 25 ops | 10 ops | 60% |
| 7Ã—7 | 49 ops | 14 ops | 71% |
| 15Ã—15 | 225 ops | 30 ops | 87% |

---

## 10. Challenge for the Reader

### Challenge 1: Separable Convolution

Implement two kernels for separable Gaussian blur:

1. **Row Pass:** Apply 1Ã—K horizontal filter
2. **Column Pass:** Apply KÃ—1 vertical filter

**Starter Code:**

```cpp
__constant__ float c_kernel_1d[MAX_KERNEL_SIZE];

__global__ void convolution_row(float* input, float* output, 
                                 int width, int height, int kernel_size) {
    // TODO: Implement horizontal convolution
    // Each thread processes one pixel
    // Only need 1D shared memory tile with halo
}

__global__ void convolution_col(float* input, float* output, 
                                 int width, int height, int kernel_size) {
    // TODO: Implement vertical convolution
    // Careful: Memory access pattern is strided!
}
```

### Challenge 2: Multi-Channel (RGB) Convolution

Extend the kernel to handle RGB images:

```cpp
// Input: 3-channel image (RGBRGBRGB... or planar RRR...GGG...BBB...)
// Apply same filter to each channel
// Consider: Which memory layout is more efficient for coalescing?
```

### Challenge 3: Benchmark and Profile

1. Use `nvprof` or Nsight Compute to measure:
   - Global memory throughput
   - Shared memory bank conflicts
   - Achieved occupancy

2. Compare your implementation against cuDNN's `cudnnConvolutionForward()`

---

## Summary

### Key Takeaways

| Concept | Lesson |
|---------|--------|
| **The Halo Problem** | Edge threads need neighbor data from adjacent blocks |
| **Constant Memory** | Perfect for small, read-only, uniform-access data (filters) |
| **Tiled Loading** | Trade increased shared memory for reduced global memory traffic |
| **Separable Filters** | Reduce $O(K^2)$ to $O(2K)$ for compatible filters |

### The Optimization Journey

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Convolution Optimization Path                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   Step 1: Naive                                                         â”‚
â”‚   â””â”€â”€ Problem: Redundant filter reads                                  â”‚
â”‚       â””â”€â”€ Solution: Constant Memory                                    â”‚
â”‚                                                                         â”‚
â”‚   Step 2: Constant Memory                                               â”‚
â”‚   â””â”€â”€ Problem: Redundant pixel reads                                   â”‚
â”‚       â””â”€â”€ Solution: Shared Memory with Halo                            â”‚
â”‚                                                                         â”‚
â”‚   Step 3: Tiled with Halo                                               â”‚
â”‚   â””â”€â”€ Problem: Large filters are slow                                  â”‚
â”‚       â””â”€â”€ Solution: Separable Convolution                              â”‚
â”‚                                                                         â”‚
â”‚   Step 4: Separable (Advanced)                                          â”‚
â”‚   â””â”€â”€ Achieved: Near-optimal memory bandwidth utilization              â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What's Next?

In the next post, we'll explore **Histogram** and **Atomics** â€” how to count and accumulate when millions of threads compete for the same memory locations.

---

## References

1. [NVIDIA CUDA C Programming Guide - Constant Memory](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#constant-memory)
2. [NVIDIA Technical Blog - Efficient Convolution](https://developer.nvidia.com/blog/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/)
3. [cuDNN Documentation](https://docs.nvidia.com/deeplearning/cudnn/developer-guide/)
4. Kirk, D. & Hwu, W. (2016). *Programming Massively Parallel Processors* - Chapter 7: Convolution
